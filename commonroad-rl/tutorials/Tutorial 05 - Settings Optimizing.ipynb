{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# Tutorial 05 - Settings Optimizing\n",
        "\n",
        "Similar to other learning-based approaches, model hyperparameters play an important role in RL. Likewise, environment configurations such as observation items or reward weights affect the outcome of RL substantially. It is therefore critical to enable a finetuning process for these different settings. We offer now three separate options to choose from:\n",
        "* optimize observation configurations\n",
        "* optimize reward configurations\n",
        "* optimize model hyperparameters\n",
        "\n",
        "In CommonRoad-RL, these are achieved with the [Optuna](https://optuna.org) package. Essentially, with Optuna\u0027s interfaces, an optimization process involves several rounds of learning processes, or \"trials\" as called in the package, each being triggered with a set of sampled configurations/hyperparameters and behaving exactly the same as a vanilla learning process. Finally, with several rounds of learning conducted, the best performing set of configurations/hyperparameters will be reported. Note that there exist different criteria for configurations and hyperparameters optimizing. A more detailed description can be found in [commonroad_rl/README.md](https://gitlab.lrz.de/ss20-mpfav-rl/commonroad-rl/-/blob/development/commonroad_rl/README.md).     \n",
        "\n",
        "We show in this tutorial an example to optimize reward configurations. The interfaces to optimize observation configurations and model hyperparamters are similar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## 0. Preparation\n",
        "\n",
        "Please make sure the training and testing data are prepared, otherwise see **Tutorial 01 - Data Preprocessing**. It is highly recommended that both **Tutorial 02 - Vanilla Learning** and **Tutorial 03 - Continual Learning** are completed first. Also, check the followings:\n",
        "* current path is at the project root `commonroad-rl`, i.e. one upper layer to the `tutorials` folder\n",
        "* interactive python kernel is triggered from the correct environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# Check current path\n",
        "%cd ..\n",
        "%pwd\n",
        "\n",
        "# Check interactive python kernel\n",
        "import sys\n",
        "sys.executable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## 1. Load RL environment and model settings\n",
        "\n",
        "Similar to a vanilla learning process, we have to specify the environment configurations and model hyperparameters beforehands. However, a major difference for configurations optimization is that besides assigning direct values, we also prepare a set of sampling settings so that later the optimizer knows what sampling method and what sampling range/candidates are used for each item. Please see `commonroad_rl/config.yaml` for details.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": "import os\nimport yaml\nimport copy\n \n# Read in environment configurations \nenv_configs \u003d {}\nwith open(\"commonroad_rl/gym_commonroad/configs.yaml\", \"r\") as config_file:\n    env_configs \u003d yaml.safe_load(config_file)[\"env_configs\"]\n\n# Save settings for later use\nlog_path \u003d \"tutorials/logs/\"\nos.makedirs(log_path, exist_ok\u003dTrue)\n\nwith open(os.path.join(log_path, \"environment_configurations.yml\"), \"w\") as config_file:\n    yaml.dump(env_configs, config_file)\n\n# Read in model hyperparameters\nhyperparams \u003d {}\nwith open(\"commonroad_rl/hyperparams/ppo2.yml\", \"r\") as hyperparam_file:\n    hyperparams \u003d yaml.safe_load(hyperparam_file)[\"commonroad-v0\"]\n    \n# Save settings for later use\nwith open(os.path.join(log_path, \"model_hyperparameters.yml\"), \"w\") as hyperparam_file:\n    yaml.dump(hyperparams, hyperparam_file)\n\n# Remove `normalize` as it will be handled explicitly later\nif \"normalize\" in hyperparams:\n    del hyperparams[\"normalize\"]\n    \n# Read in sampling settings for reward configurations\nsampling_settings_reward_configs \u003d {}\nwith open(\"commonroad_rl/gym_commonroad/configs.yaml\", \"r\") as config_file:\n    sampling_settings_reward_configs \u003d yaml.safe_load(config_file)[\"sampling_setting_reward_configs\"]"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## 2. Create training and testing environments\n",
        "\n",
        "Likewise, training and testing environments are to be prepared for settings optimizing. However, since we are going to sample and optimize the reward configurations later, we do not create the environments directly at this point, but a callable function and pass it to the optimizer interface from the [Optuna](https://optuna.org) package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "from stable_baselines.bench import Monitor\n",
        "from stable_baselines.common.vec_env import DummyVecEnv, VecNormalize\n",
        "\n",
        "import commonroad_rl.gym_commonroad\n",
        "\n",
        "# Prepare a low-level environment maker\n",
        "meta_scenario_path \u003d \"tutorials/data/highD/pickles/meta_scenario\"\n",
        "training_data_path \u003d \"tutorials/data/highD/pickles/problem_train\"\n",
        "testing_data_path \u003d \"tutorials/data/highD/pickles/problem_test\"\n",
        "\n",
        "def make_env(**env_kwargs):\n",
        "    def _func():\n",
        "        # Create the environment\n",
        "        env \u003d gym.make(\"commonroad-v0\", \n",
        "                       meta_scenario_path\u003dmeta_scenario_path,\n",
        "                       train_reset_config_path\u003d training_data_path,\n",
        "                       test_reset_config_path\u003d testing_data_path,\n",
        "                       **env_kwargs)\n",
        "\n",
        "        # Wrap the environment with a monitor to keep an record of the learning process\n",
        "        info_keywords\u003dtuple([\"is_collision\", \\\n",
        "                             \"is_time_out\", \\\n",
        "                             \"is_off_road\", \\\n",
        "                             \"is_friction_violation\", \\\n",
        "                             \"is_goal_reached\"])\n",
        "        env \u003d Monitor(env, log_path + \"infos\", info_keywords\u003dinfo_keywords)\n",
        "        return env\n",
        "    return _func\n",
        "\n",
        "# Prepare a callable function for the optimizer\n",
        "def create_env(**env_kwargs):\n",
        "    # Vectorize the environment\n",
        "    env \u003d DummyVecEnv([make_env(**env_kwargs)])\n",
        "\n",
        "    # Normalize observations and rewards as required\n",
        "    if \"test_env\" not in env_kwargs or env_kwargs[\"test_env\"] is False:\n",
        "        # Normalize observations and rewards during training\n",
        "        env \u003d VecNormalize(env, norm_obs\u003dTrue, norm_reward\u003dTrue)\n",
        "    else:\n",
        "        # Normalize only observations during testing\n",
        "        env \u003d VecNormalize(env, norm_obs\u003dTrue, norm_reward\u003dFalse)\n",
        "\n",
        "    return env\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## 3. Create a model\n",
        "\n",
        "In addition, we do not create a model explicitly but prepare a callable function and pass it to the optimizer later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from stable_baselines import PPO2\n",
        "\n",
        "def create_model(hyperparams, env_configs):\n",
        "    return PPO2(env\u003dcreate_env(**env_configs), **hyperparams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## 4. Assemble an evaluation callback with optimization criteria\n",
        "\n",
        "During an optimization process, it is important to assess how good or bad a set of sampled configurations has performed. This is done with an evaluation callback, which will be appended to every learning trial. In `commonroad_rl/utils_run/callbacks.py`, there are specific callback functions defined for reward configurations, observation configurations, and model hyperparameters. In the following, we show how the callback for reward configurations are established for later use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from stable_baselines.common.callbacks import EvalCallback\n",
        "from stable_baselines.common.vec_env import sync_envs_normalization, VecEnv\n",
        "\n",
        "class RewardConfigsTrialEvalCallback(EvalCallback):\n",
        "    def __init__(\n",
        "        self,\n",
        "        eval_env,\n",
        "        trial,\n",
        "        n_eval_episodes\u003d5,\n",
        "        eval_freq\u003d10000,\n",
        "        log_path\u003dNone,\n",
        "        best_model_save_path\u003dNone,\n",
        "        deterministic\u003dTrue,\n",
        "        verbose\u003d1,\n",
        "    ):\n",
        "        super(RewardConfigsTrialEvalCallback, self).__init__(\n",
        "            eval_env\u003deval_env,\n",
        "            n_eval_episodes\u003dn_eval_episodes,\n",
        "            eval_freq\u003deval_freq,\n",
        "            deterministic\u003ddeterministic,\n",
        "            verbose\u003dverbose,\n",
        "        )\n",
        "        self.trial \u003d trial\n",
        "        self.eval_idx \u003d 0\n",
        "        self.is_pruned \u003d False\n",
        "        self.lowest_mean_cost \u003d np.inf\n",
        "        self.last_mean_cost \u003d np.inf\n",
        "        self.cost \u003d 0.0\n",
        "\n",
        "        # Save best model into `($best_model_save_path)/trial_($trial_number)/best_model.zip`\n",
        "        if best_model_save_path is not None:\n",
        "            self.best_model_save_path \u003d os.path.join(\n",
        "                best_model_save_path, \"trial_\" + str(trial.number), \"best_model\"\n",
        "            )\n",
        "            os.makedirs(self.best_model_save_path, exist_ok\u003dTrue)\n",
        "        else:\n",
        "            self.best_model_save_path \u003d best_model_save_path\n",
        "\n",
        "        # Log evaluation information into `($log_path)/trial_($trial_number)/evaluations.npz`\n",
        "        self.evaluation_timesteps \u003d []\n",
        "        self.evaluation_costs \u003d []\n",
        "        self.evaluation_lengths \u003d []\n",
        "        if log_path is not None:\n",
        "            self.log_path \u003d os.path.join(\n",
        "                log_path, \"trial_\" + str(trial.number), \"evaluations\"\n",
        "            )\n",
        "            os.makedirs(os.path.dirname(self.log_path), exist_ok\u003dTrue)\n",
        "        else:\n",
        "            self.log_path \u003d log.path\n",
        "\n",
        "    def _on_step(self):\n",
        "        if self.eval_freq \u003e 0 and self.n_calls % self.eval_freq \u003d\u003d 0:\n",
        "            def evaluate_policy_configs(\n",
        "                model,\n",
        "                env,\n",
        "                n_eval_episodes\u003d10,\n",
        "                render\u003dFalse,\n",
        "                deterministic\u003dTrue,\n",
        "                callback\u003dNone,\n",
        "            ):\n",
        "                \"\"\"\n",
        "                Runs policy for `n_eval_episodes` episodes and returns cost for optimization.\n",
        "                This is made to work only with one env.\n",
        "\n",
        "                :param model: (BaseRLModel) The RL agent you want to evaluate.\n",
        "                :param env: (gym.Env or VecEnv) The gym environment. In the case of a `VecEnv`, this must contain only one environment.\n",
        "                :param n_eval_episodes: (int) Number of episode to evaluate the agent\n",
        "                :param deterministic: (bool) Whether to use deterministic or stochastic actions\n",
        "                :param render: (bool) Whether to render the environment or not\n",
        "                :param callback: (callable) callback function to do additional checks, called after each step.\n",
        "                :return: ([float], [int]) list of episode costs and lengths\n",
        "                \"\"\"\n",
        "                if isinstance(env, VecEnv):\n",
        "                    assert (\n",
        "                        env.num_envs \u003d\u003d 1\n",
        "                    ), \"You must pass only one environment when using this function\"\n",
        "\n",
        "                episode_costs \u003d []\n",
        "                episode_lengths \u003d []\n",
        "                for _ in range(n_eval_episodes):\n",
        "                    obs \u003d env.reset()\n",
        "                    done, info, state \u003d False, None, None\n",
        "\n",
        "                    # Record required information\n",
        "                    # Since vectorized environments get reset automatically after each episode,\n",
        "                    # we have to keep a copy of the relevant states here.\n",
        "                    # See https://stable-baselines.readthedocs.io/en/master/guide/vec_envs.html for more details.\n",
        "                    episode_length \u003d 0\n",
        "                    episode_cost \u003d 0.0\n",
        "                    episode_is_time_out \u003d []\n",
        "                    episode_is_collision \u003d []\n",
        "                    episode_is_off_road \u003d []\n",
        "                    episode_is_goal_reached \u003d []\n",
        "                    episode_is_friction_violation \u003d []\n",
        "                    while not done:\n",
        "                        action, state \u003d model.predict(\n",
        "                            obs, state\u003dstate, deterministic\u003ddeterministic\n",
        "                        )\n",
        "                        obs, reward, done, info \u003d env.step(action)\n",
        "\n",
        "                        episode_length +\u003d 1\n",
        "                        episode_is_time_out.append(info[-1][\"is_time_out\"])\n",
        "                        episode_is_collision.append(info[-1][\"is_collision\"])\n",
        "                        episode_is_off_road.append(info[-1][\"is_off_road\"])\n",
        "                        episode_is_goal_reached.append(info[-1][\"is_goal_reached\"])\n",
        "                        episode_is_friction_violation.append(info[-1][\"is_friction_violation\"])\n",
        "\n",
        "                        if callback is not None:\n",
        "                            callback(locals(), globals())\n",
        "                        if render:\n",
        "                            env.render()\n",
        "\n",
        "                    # Calculate cost for optimization from state information\n",
        "                    normalized_episode_length \u003d (\n",
        "                        episode_length / info[-1][\"max_episode_time_steps\"]\n",
        "                    )\n",
        "                    if episode_is_time_out[-1]:\n",
        "                        episode_cost +\u003d 10.0 * (1 / normalized_episode_length)\n",
        "                    if episode_is_collision[-1]:\n",
        "                        episode_cost +\u003d 10.0 * (1 / normalized_episode_length)\n",
        "                    if episode_is_off_road[-1]:\n",
        "                        episode_cost +\u003d 10.0 * (1 / normalized_episode_length)\n",
        "                    if episode_is_friction_violation[-1]:\n",
        "                        episode_cost +\u003d 10.0 * (1 / normalized_episode_length)\n",
        "                    if episode_is_goal_reached[-1]:\n",
        "                        episode_cost -\u003d 10.0 * normalized_episode_length\n",
        "\n",
        "                    episode_costs.append(episode_cost)\n",
        "                    episode_lengths.append(episode_length)\n",
        "\n",
        "                return episode_costs, episode_lengths\n",
        "\n",
        "            sync_envs_normalization(self.training_env, self.eval_env)\n",
        "            episode_costs, episode_lengths \u003d evaluate_policy_configs(\n",
        "                self.model,\n",
        "                self.eval_env,\n",
        "                n_eval_episodes\u003dself.n_eval_episodes,\n",
        "                render\u003dself.render,\n",
        "                deterministic\u003dself.deterministic,\n",
        "            )\n",
        "\n",
        "            mean_cost, std_cost \u003d np.mean(episode_costs), np.std(episode_costs)\n",
        "            mean_length, std_length \u003d np.mean(episode_lengths), np.std(episode_lengths)\n",
        "            self.last_mean_cost \u003d mean_cost\n",
        "\n",
        "            if self.verbose \u003e 0:\n",
        "                print(\"Evaluating at learning time step: {}\".format(self.num_timesteps))\n",
        "                print(\"Cost mean: {:.2f}, std: {:.2f}\".format(mean_cost, std_cost))\n",
        "                print(\"Length mean: {:.2f}, std: {:.2f}\".format(mean_length, std_length))\n",
        "\n",
        "            if self.log_path is not None:\n",
        "                self.evaluation_timesteps.append(self.num_timesteps)\n",
        "                self.evaluation_costs.append(episode_costs)\n",
        "                self.evaluation_lengths.append(episode_lengths)\n",
        "                np.savez(\n",
        "                    self.log_path,\n",
        "                    timesteps\u003dself.evaluation_timesteps,\n",
        "                    episode_costs\u003dself.evaluation_costs,\n",
        "                    episode_lengths\u003dself.evaluation_lengths,\n",
        "                )\n",
        "\n",
        "            if mean_cost \u003c self.lowest_mean_cost:\n",
        "                self.lowest_mean_cost \u003d mean_cost\n",
        "                if self.best_model_save_path is not None:\n",
        "                    self.model.save(self.best_model_save_path)\n",
        "                # Trigger callback if needed\n",
        "                if self.callback is not None:\n",
        "                    return self._on_event()\n",
        "\n",
        "            # Report trial results\n",
        "            self.eval_idx +\u003d 1\n",
        "            self.cost \u003d self.lowest_mean_cost\n",
        "            self.trial.report(self.cost, self.eval_idx)\n",
        "            # Prune trial if need\n",
        "            if self.trial.should_prune():\n",
        "                self.is_pruned \u003d True\n",
        "                return False\n",
        "        return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## 5. Formalize the optimization objective and process\n",
        "\n",
        "Also different from a regular learning process, we have to define an objective function which helps sample the set of configurations to be used in the upcoming trial, call the functions for the model, environments and callbacks, and start a trial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "def objective_reward_configs(trial):\n",
        "    # Sample reward configurations according to settings\n",
        "    sampled_reward_configs \u003d {}\n",
        "    for key, value in sampling_settings_reward_configs.items():\n",
        "        method, interval \u003d next(iter(value.items()))\n",
        "        if method \u003d\u003d \"categorical\":\n",
        "            sampled_reward_configs[key] \u003d trial.suggest_categorical(key, interval)\n",
        "        elif method \u003d\u003d \"uniform\":\n",
        "            sampled_reward_configs[key] \u003d trial.suggest_uniform(key, interval[0], interval[1])\n",
        "        elif method \u003d\u003d \"loguniform\":\n",
        "            sampled_reward_configs[key] \u003d trial.suggest_loguniform(key, interval[0], interval[1])\n",
        "        else:\n",
        "            print(\"Sampling method \" + method + \" not supported for \" + key)\n",
        "    \n",
        "    # Update environment configurations\n",
        "    env_configs.update(sampled_reward_configs)\n",
        "    \n",
        "    # Save data for later inspection\n",
        "    tmp_path \u003d os.path.join(log_path, \"trial_\" + str(trial.number))\n",
        "    os.makedirs(tmp_path, exist_ok\u003dTrue)\n",
        "    with open(os.path.join(tmp_path, \"environment_configurations.yml\"), \"w\") as f:\n",
        "        yaml.dump(env_configs, f)\n",
        "    \n",
        "    model \u003d create_model(hyperparams, env_configs)\n",
        "    testing_env \u003d create_env(test_env\u003dTrue, **env_configs)\n",
        "\n",
        "    reward_configs_eval_callback \u003d RewardConfigsTrialEvalCallback(testing_env,\n",
        "                                                                  trial,\n",
        "                                                                  n_eval_episodes\u003d3,\n",
        "                                                                  eval_freq\u003d500,\n",
        "                                                                  log_path\u003dlog_path,\n",
        "                                                                  best_model_save_path\u003dlog_path,\n",
        "                                                                  deterministic\u003dTrue,\n",
        "                                                                  verbose\u003d1)\n",
        "    # Conduct a learning trial\n",
        "    try:\n",
        "        n_timesteps \u003d 3000\n",
        "        model.learn(n_timesteps, callback\u003dreward_configs_eval_callback)\n",
        "        # Free memory\n",
        "        model.env.close()\n",
        "        testing_env.close()\n",
        "    # Catch NaN from bad random configurations\n",
        "    except AssertionError:\n",
        "        # Free memory\n",
        "        model.env.close()\n",
        "        testing_env.close()\n",
        "        raise optuna.exceptions.TrialPruned()\n",
        "    \n",
        "    # Record trial results\n",
        "    is_pruned \u003d reward_configs_eval_callback.is_pruned\n",
        "    cost \u003d reward_configs_eval_callback.cost\n",
        "    del model.env, testing_env\n",
        "    del model\n",
        "    if is_pruned:\n",
        "        raise optuna.exceptions.TrialPruned()\n",
        "    return cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## 6. Trigger the optimization process and save the best\n",
        "\n",
        "Finally, we are ready to trigger the optimization process. For such, we first create a `study` object from the Optuna package with the designated sampler and pruner, and then call the `optimize` member function to start the overall process, passing in the objective function defined above. Please see the [Optuna examples](https://optuna.org/#code_examples) for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "from optuna.samplers import RandomSampler\n",
        "from optuna.pruners import MedianPruner\n",
        "\n",
        "# Create a study object on reward configurations from Optuna\n",
        "reward_configs_study \u003d optuna.create_study(sampler\u003dRandomSampler(), pruner\u003dMedianPruner())\n",
        "\n",
        "# Start optimizing\n",
        "reward_configs_study.optimize(objective_reward_configs, n_trials\u003d5, n_jobs\u003d1)\n",
        "\n",
        "# Access and record the best performing set of configurations after all trials\n",
        "with open(os.path.join(log_path, \"report_reward_configs_study.yaml\"), \"w\") as f:\n",
        "    yaml.dump(reward_configs_study.best_trial.params, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Now in `tutorials/logs`, there should be a resulting best `.yaml` file and several `trail_*` folders recording the information for each of the optimization trials. These are useful for inspection and reuse in other subsequent learnings. For a detailed diretory description, please refer to the [commonroad_rl/README.md](https://gitlab.lrz.de/ss20-mpfav-rl/commonroad-rl/-/tree/development/commonroad_rl) file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}